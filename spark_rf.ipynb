{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = '/usr/lib/jvm/java-8-openjdk-amd64/'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = 'pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_HOME\"] = '/home/sony/Oleg/hse/big_data/spark-2.2.0-bin-hadoop2.7/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(os.environ['SPARK_HOME']+\"/python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.environ['SPARK_HOME']+\"/python/lib/py4j-0.10.4-src.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import py4j\n",
    "from pyspark import SparkContext, SparkConf, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf().setMaster(\"local[8]\")\n",
    "        .setAppName(\"ML demo\")\n",
    "        .set(\"spark.executor.memory\", \"2g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionModel, LinearRegressionWithSGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sony/Oleg/hse/big_data/spark-2.2.0-bin-hadoop2.7//python/pyspark/mllib/regression.py:281: UserWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.928638123469\n"
     ]
    }
   ],
   "source": [
    "data=[\n",
    "    LabeledPoint(0.0,[0.0]),\n",
    "    LabeledPoint(1.0,[1.0]),\n",
    "    LabeledPoint(3.0,[2.0]),\n",
    "    LabeledPoint(2.0,[3.0])\n",
    "]\n",
    "lrm=LinearRegressionWithSGD.train(sc.parallelize(data),iterations=10,initialWeights=np.array([1.0]))\n",
    "print(lrm.predict(np.array([1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlcontext.read.format(\n",
    "    'com.databricks.spark.csv').options(\n",
    "    header='true').load('/home/sony/Oleg/hse/big_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId='1', Survived='0', Pclass='3', Name='Braund, Mr. Owen Harris', Sex='male', Age='22', SibSp='1', Parch='0', Ticket='A/5 21171', Fare='7.25', Cabin=None, Embarked='S'),\n",
       " Row(PassengerId='2', Survived='1', Pclass='1', Name='Cumings, Mrs. John Bradley (Florence Briggs Thayer)', Sex='female', Age='38', SibSp='1', Parch='0', Ticket='PC 17599', Fare='71.2833', Cabin='C85', Embarked='C'),\n",
       " Row(PassengerId='3', Survived='1', Pclass='3', Name='Heikkinen, Miss. Laina', Sex='female', Age='26', SibSp='0', Parch='0', Ticket='STON/O2. 3101282', Fare='7.925', Cabin=None, Embarked='S')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to one-hot encode using build-in functions and remove None using udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Embarked values =  [Row(Embarked='Q'), Row(Embarked='C'), Row(Embarked='S'), Row(Embarked='')]\n",
      "\n",
      "New df =  DataFrame[PassengerId: string, Survived: string, Pclass: string, Name: string, Sex: string, Age: string, SibSp: string, Parch: string, Ticket: string, Fare: string, Cabin: string, Embarked: string, EmbarkedIndex: double, EmbarkedVec: vector]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark.sql import types\n",
    "\n",
    "\n",
    "\n",
    "def Embarked_transform(x):\n",
    "    if x != None:\n",
    "        return x\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "my_udf = udf(Embarked_transform, types.StringType())\n",
    "df = df.withColumn('Embarked', my_udf(df['Embarked']))\n",
    "print(\"New Embarked values = \", df.select('Embarked').distinct().collect())# print values of new column\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(inputCol=\"EmbarkedIndex\", outputCol=\"EmbarkedVec\")\n",
    "df_t = encoder.transform(indexed)\n",
    "print(\"\\nNew df = \",df_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hw udf feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Cabin values =  [Row(Cabin='F'), Row(Cabin='FG'), Row(Cabin='NA'), Row(Cabin='E'), Row(Cabin='T'), Row(Cabin='B'), Row(Cabin='D'), Row(Cabin='EF'), Row(Cabin='C'), Row(Cabin='A'), Row(Cabin='G')]\n",
      "\n",
      "New df =  DataFrame[PassengerId: string, Survived: string, Pclass: string, Name: string, Sex: string, Age: string, SibSp: string, Parch: string, Ticket: string, Fare: string, Cabin: string, Embarked: string, EmbarkedIndex: double, EmbarkedVec: vector, CabinIndex: double, CabinVec: vector]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def cabin_transform(x):\n",
    "    \"\"\"select unique letters from the feature\n",
    "    \"\"\"\n",
    "    if x != None:\n",
    "        uq_letters = set(\"\".join(re.findall(\"[a-zA-Z]+\", x)))\n",
    "        uq_letters = sorted(list(uq_letters))\n",
    "        return ''.join(uq_letters)\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "cabin_udf = udf(cabin_transform, types.StringType())\n",
    "df_t = df_t.withColumn('Cabin', cabin_udf(df_t['Cabin']))\n",
    "print(\"New Cabin values = \", df_t.select('Cabin').distinct().collect())\n",
    "\n",
    "stringIndexerC = StringIndexer(inputCol=\"Cabin\", outputCol=\"CabinIndex\")\n",
    "modelC = stringIndexerC.fit(df_t)\n",
    "indexedC = modelC.transform(df_t)\n",
    "encoderC = OneHotEncoder(inputCol=\"CabinIndex\", outputCol=\"CabinVec\")\n",
    "df_t = encoderC.transform(indexedC)\n",
    "print(\"\\nNew df = \", df_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Building data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_age(str_age):\n",
    "    try:\n",
    "        return float(str_age)\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transf(r):\n",
    "    return LabeledPoint(\n",
    "        int(r.Survived),\n",
    "        [\n",
    "            int(r.Pclass),\n",
    "            r.Sex == 'male',\n",
    "            float(r.Fare),\n",
    "            int(r.SibSp),\n",
    "            int(r.Parch),\n",
    "            parse_age(r.Age),\n",
    "            \n",
    "            parse_age(r.Age) > 18,\n",
    "            int(r.SibSp) > 1,\n",
    "            int(r.Pclass) > 3,\n",
    "            int(r.Parch) > 0,\n",
    "            \n",
    "        ] + list(r.EmbarkedVec.toArray())+ list(r.CabinVec.toArray())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = df_t.rdd.map(transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[178] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.mllib.tree import GradientBoostedTrees,DecisionTree\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "models = {}\n",
    "models[\"Random_forest\"] = RandomForest.trainClassifier(train, numClasses=2,\n",
    "                             categoricalFeaturesInfo={},\n",
    "                            numTrees=100)\n",
    "models[\"SVM\"] = SVMWithSGD.train(train, regType='l2',regParam=0.01, iterations=200,intercept=True)\n",
    "models[\"Gradient_Boosted_Trees\"] = GradientBoostedTrees.trainClassifier(train,\n",
    "                                                                      categoricalFeaturesInfo={}, numIterations=200)\n",
    "models[\"Decision_tree\"] = DecisionTree.trainClassifier(train, numClasses=2,categoricalFeaturesInfo={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc(model, test):\n",
    "    values = test.map(lambda x: x.features)\n",
    "    yhat = model.predict(values)\n",
    "    y = test.map(lambda x: x.label)\n",
    "    comp = yhat.zip(y)\n",
    "    errors = comp.map(lambda x: abs(x[0]-x[1]))\n",
    "    return 1-errors.sum()/errors.count()\n",
    "\n",
    "def f1_score(model, test):\n",
    "    values = test.map(lambda x: x.features)\n",
    "    yhat = model.predict(values)\n",
    "    y = test.map(lambda x: x.label)\n",
    "    comp = yhat.zip(y)\n",
    "    tp = comp.map(lambda x: (x[0] == 1 and x[1] == 1)).sum()\n",
    "    prec = tp/yhat.sum()\n",
    "    recall = tp/y.sum()\n",
    "    return 2 * prec*recall/(prec + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Gradient_Boosted_Trees:\n",
      "accuracy =  0.8065693430656934\n",
      "f1 score =  0.7336683417085428\n",
      "Testing Random_forest:\n",
      "accuracy =  0.8065693430656934\n",
      "f1 score =  0.7135135135135134\n",
      "Testing SVM:\n",
      "accuracy =  0.6313868613138687\n",
      "f1 score =  0.12173913043478259\n",
      "Testing Decision_tree:\n",
      "accuracy =  0.781021897810219\n",
      "f1 score =  0.7\n"
     ]
    }
   ],
   "source": [
    "for name,model in models.items():\n",
    "    print(\"Testing \"+name+\":\")\n",
    "    print(\"accuracy = \",acc(model,test))\n",
    "    print(\"f1 score = \",f1_score(model,test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# добавить 5 новых фичей\n",
    "# 3 фичи высчитываются из имеющихся\n",
    "# хотя бы одна использует udf\n",
    "\n",
    "# попробовать 3 новых модели\n",
    "\n",
    "# f1 меру"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
